{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at mwz/UrduBert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mwz/UrduBert\")\n",
    "model = AutoModel.from_pretrained(\"mwz/UrduBert\")\n",
    "\n",
    "def get_embedding(text, pooling_type=\"cls\"):\n",
    "    # Tokenize the input text and convert to PyTorch tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Forward pass through the model to get the outputs (hidden states)\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the last hidden state (embeddings)\n",
    "    last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "    # Choose embedding type based on pooling_type\n",
    "    if pooling_type == \"cls\":\n",
    "        # Take the [CLS] token embedding\n",
    "        cls_embedding = last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "        return cls_embedding.squeeze(0)  # Return a tensor of shape [hidden_size]\n",
    "    \n",
    "    elif pooling_type == \"mean\":\n",
    "        # Mean-pool the token embeddings (excluding padding tokens)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mean_pooling = torch.sum(last_hidden_state * attention_mask.unsqueeze(-1), dim=1) / torch.sum(attention_mask.unsqueeze(-1), dim=1)\n",
    "        return mean_pooling.squeeze(0)  # Return a tensor of shape [hidden_size]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"pooling_type must be either 'cls' or 'mean'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1206,  0.5497,  0.6291, -0.6541, -0.1881, -0.6269, -0.0763,  0.7037,\n",
       "          0.8596, -0.1699,  0.1436,  0.6277, -0.7546, -0.7827,  0.1517, -0.6952,\n",
       "         -1.4332, -0.6462,  0.3964, -0.2134,  1.0889, -0.6394,  1.2095,  1.6724,\n",
       "          0.7817, -0.4934, -0.1990, -0.2254, -0.1559,  0.5399, -0.6145, -0.5507,\n",
       "          0.2027,  1.1732,  0.4317,  0.7345, -0.8598, -0.8404,  0.4689,  0.4179,\n",
       "         -1.0180, -0.3925,  0.2917, -0.5717, -0.7609,  0.5619, -0.5860,  0.0342,\n",
       "         -0.6626,  1.1468,  0.7482,  1.1459, -0.1212,  0.0868,  0.2955, -0.6337,\n",
       "         -0.0282,  0.6349,  0.2171, -0.1347, -0.4041,  0.1823, -0.1671,  0.3206,\n",
       "         -0.8347, -1.7990,  0.8173, -0.7448, -0.1045, -0.1358,  0.2773,  0.0843,\n",
       "          0.8245,  0.8428, -0.8805, -0.6240,  0.3297,  0.6343, -0.4761,  0.8343,\n",
       "          0.6483, -0.4176, -0.1682,  0.9472, -0.0649,  0.2426,  0.6476,  0.2449,\n",
       "          0.0163, -0.3859,  0.0501,  0.2681,  0.2552,  0.0155,  0.5071, -0.6122,\n",
       "         -0.9226, -0.4496, -0.6642, -1.6866,  0.2643, -0.9385, -0.0184,  0.9744,\n",
       "         -0.1749,  1.0878, -0.4132, -1.7689, -0.3781,  1.0324,  0.0107,  0.3676,\n",
       "         -0.3076,  0.7004,  0.8956,  0.0392, -0.8763,  0.0316,  0.0325, -0.4140,\n",
       "         -0.2296,  0.1196, -0.2174,  0.6633,  0.3092, -0.5635, -0.2373, -0.3674,\n",
       "         -0.1599, -0.2286,  0.4945,  0.2636,  0.5467, -0.0273, -0.1974, -0.3401,\n",
       "         -0.0682,  0.3295, -1.0030,  0.1166, -1.1736,  0.2958,  0.5549, -0.0854,\n",
       "         -0.5275,  0.6056, -0.2781, -0.8469, -0.1054,  1.2161, -0.9088,  0.2168,\n",
       "          1.0778,  0.7001,  0.0111, -0.5869, -0.5121, -0.2532, -0.1221,  1.2858,\n",
       "          0.9746,  0.2656, -0.3288,  0.2441,  0.2260, -1.2007, -0.6073, -0.7490,\n",
       "         -0.2662, -0.2211, -0.0789, -0.4319,  0.4926,  0.2753,  0.1712,  0.0380,\n",
       "          0.9210, -0.4345,  0.5687, -1.1846, -1.7012,  0.4662,  0.3215, -0.5559,\n",
       "         -0.6079, -0.2008, -0.5866,  0.1909,  0.1779, -0.2135,  1.4211,  0.3158,\n",
       "         -1.8144,  0.7035,  0.6467,  0.6065, -1.0361,  0.2761,  0.4295,  0.6603,\n",
       "          0.0885,  0.9275,  0.2198, -1.0228,  0.1163,  0.6198,  0.4277, -0.5571,\n",
       "          0.1896, -0.7330, -0.2720, -0.1426,  0.6904, -0.4522,  0.6246,  0.4153,\n",
       "         -0.8635,  0.1955, -1.2866, -0.3383, -0.4872, -1.1704, -0.2794,  0.3652,\n",
       "         -0.0997, -0.5973, -1.0921, -0.0865,  0.2123,  1.4237,  0.0728,  0.8040,\n",
       "         -1.4607,  0.1656, -0.7318, -1.1104, -0.7601,  0.0677,  0.7594,  0.5260,\n",
       "         -0.4144, -0.4134, -0.3490,  0.7769, -0.0216, -0.3940, -0.3204,  0.3872,\n",
       "          0.0815, -0.2255,  0.4503,  0.0244, -0.6369, -0.6601, -0.1160, -1.0601,\n",
       "          0.3893, -0.5503,  0.0459, -0.2185, -0.1571,  0.6150,  1.4298,  0.8048,\n",
       "         -0.6657, -0.8229,  1.6053, -0.8665, -1.6679,  0.0954,  0.4154, -0.9179,\n",
       "          0.3502,  1.1367, -0.3745,  1.2843,  0.3136,  1.7802, -0.9403, -0.2951,\n",
       "         -0.2397, -0.5796, -0.6216,  0.3115, -0.8848, -0.3236, -0.9192, -0.1729,\n",
       "          0.8581, -0.2264, -0.2004,  0.2691,  1.3426, -0.2185, -0.8009,  0.1212,\n",
       "         -0.5591,  0.8138,  0.6669, -0.1986,  1.1274,  1.8100, -0.1126,  0.7782,\n",
       "          0.3242,  0.3106,  0.3521,  0.1719, -1.0793,  0.6405, -0.2453,  0.8158,\n",
       "          0.2747,  0.5299,  0.8196, -0.1724,  0.1422,  0.0184, -0.4444, -0.4091,\n",
       "          0.4023,  0.4961, -0.1148,  0.5017,  0.0699,  0.9449, -0.1816, -0.3657,\n",
       "          0.1265, -0.1248, -0.7096, -0.6352, -0.0473, -0.1574,  0.1105, -1.1167,\n",
       "          0.4703,  0.8234, -0.2386,  0.0982, -1.3796, -0.2593, -1.0726,  0.4834,\n",
       "          0.4296,  0.8084,  0.1443, -0.5751, -0.9429, -0.6210,  0.4808,  0.7409,\n",
       "         -0.2169, -0.0848,  1.7162,  0.3235,  1.0931,  0.0268, -0.6981, -0.7240,\n",
       "          1.0345, -0.7053,  0.2841, -0.7969, -1.0017,  0.1361,  0.5505,  0.5213,\n",
       "          0.4013,  0.6458,  1.2829,  0.7039, -0.5302,  0.9746,  1.1216, -1.0713,\n",
       "         -1.8222, -0.2408, -0.3771,  0.5264, -1.0994,  0.2406,  0.6482, -0.0960,\n",
       "          0.4847, -1.1289, -0.9875, -0.1150,  0.1573, -0.3020,  0.3619,  0.4129,\n",
       "         -0.1773, -0.2303, -1.3631,  1.5581,  1.2058,  0.5964,  0.0339,  0.3837,\n",
       "         -0.9580, -0.1394, -0.1279, -0.0944, -0.4033,  0.7333,  0.4794, -0.0311,\n",
       "          1.0560, -0.1934, -0.0618, -1.3375,  1.2496,  0.1335, -0.3681,  0.6195,\n",
       "          0.0215,  1.1323, -0.7007,  1.0689, -0.6635,  0.1137, -0.1951, -0.4447,\n",
       "         -0.4387,  0.2988, -0.0020, -0.8403,  0.3233, -0.3492,  0.3417,  0.8194,\n",
       "          0.3919,  0.8444, -0.1064, -0.2703,  0.9470, -0.8979,  0.7808, -0.5623,\n",
       "         -0.8579,  0.5287,  0.3412, -0.5784,  0.3565, -0.1928,  0.1621,  0.0700,\n",
       "         -0.1947,  0.0956,  0.3035,  0.0617,  0.0153,  0.1217,  1.6103,  0.2046,\n",
       "          0.4338, -0.8196, -1.1943, -0.3219,  0.3306, -0.0786, -0.4729, -0.3706,\n",
       "         -1.5563, -0.9496, -0.6462,  0.0295, -0.3511, -0.4514, -0.4583, -1.0391,\n",
       "          0.6523, -0.0417, -1.0122, -0.4909,  0.2849, -0.1999,  0.1840, -0.6138,\n",
       "          0.8846, -0.2622, -0.0608, -0.0767,  1.1862, -0.1251,  0.0907, -0.4125,\n",
       "         -0.1552, -0.9949, -0.5863, -0.4580,  0.5755,  0.5734, -0.0547,  0.7155,\n",
       "          0.6398,  0.0637, -0.8021, -0.1614,  0.3686, -0.1110, -0.8794,  0.2801,\n",
       "         -0.0201, -0.6143, -0.1942, -0.3578, -1.4523,  0.4494,  0.7172,  0.1403,\n",
       "         -0.8083, -0.5297, -0.4261,  0.6662,  0.5148,  0.3674,  0.6973,  0.0527,\n",
       "          0.2902,  1.0093, -0.0104, -0.8381, -0.2460,  0.1009, -1.0702,  0.2844,\n",
       "          0.8225, -0.3556, -0.4232,  0.6108, -1.1335, -0.6624,  0.5903,  0.6259,\n",
       "         -1.6620, -0.1965,  0.8147,  0.0780,  0.8011, -0.5478, -1.0550,  0.6125,\n",
       "          0.4289, -0.9255,  0.4156,  0.3201, -1.1622,  0.2937,  0.3009, -0.6078,\n",
       "         -0.2149,  0.1109,  0.2387, -0.9805,  0.7430, -0.1774,  0.9424, -0.0518,\n",
       "          0.5822,  0.2570, -1.2966,  1.2241, -0.9396, -0.0511,  0.3478,  0.7774,\n",
       "          0.8315, -0.5058,  0.1221, -0.3028,  1.2480, -0.2657,  0.0388, -0.5580,\n",
       "         -0.7621, -0.4607,  0.4194, -0.4562, -0.2203,  0.5874, -0.8169,  1.0169,\n",
       "         -0.3658, -0.8454,  0.0749, -0.5187,  1.2294,  0.5214,  0.1962, -0.2051,\n",
       "          0.2041, -0.3056,  0.9574,  0.2521, -0.8467, -1.6004,  1.2542, -0.2362,\n",
       "         -0.0551, -1.4703,  0.7384, -0.4281, -0.0758,  1.2862, -0.0393,  0.4641,\n",
       "          0.3552,  0.1626, -0.5621,  0.4808,  0.2370,  0.1826, -0.0623, -0.2057,\n",
       "         -1.1882, -0.8986,  0.9116,  0.0926,  0.7825, -0.1432,  0.6680,  0.2364,\n",
       "          0.0249,  0.8652,  0.2142, -0.3393, -0.0272,  0.4074,  0.5735,  0.5692,\n",
       "          0.7311, -0.1928,  0.7412,  0.7544, -0.4262, -0.5981,  0.2322, -0.3534,\n",
       "         -1.8333, -1.8371,  0.9575, -0.1973,  0.5939, -0.6142,  0.2315, -0.9717,\n",
       "          0.3912,  0.1451, -1.0305, -0.1445,  0.2389,  0.9710,  0.5444, -0.2635,\n",
       "          0.3483,  0.0742,  1.4645, -0.3759, -0.1646,  0.0930,  0.0255, -0.0476,\n",
       "         -0.0112,  0.0678, -0.0506, -0.3080,  1.0755, -0.6486,  0.7125, -0.2765,\n",
       "          0.3336,  0.8739, -0.4377,  0.3627,  1.7648, -0.0735, -0.0030, -0.4233,\n",
       "          0.8803, -0.7926, -0.1321, -0.0458,  1.2795,  0.2598,  1.0471, -0.5599,\n",
       "         -0.0357, -0.2533,  0.6351, -0.1723,  0.4438, -1.5375, -1.0211, -0.3408,\n",
       "          0.1880, -0.2697, -1.5148,  0.8586,  0.5020, -1.5326,  0.0273,  0.2504,\n",
       "          0.1103,  0.9600,  0.4004,  0.3186,  0.3251,  0.3157,  0.3222, -0.4066,\n",
       "          0.1127, -0.1909,  0.1096,  0.1970,  0.2075,  0.7371, -0.0529, -0.2029,\n",
       "          0.9504, -0.2898, -0.0068,  0.7625,  0.8809, -0.0361, -0.4401, -0.3701,\n",
       "         -0.1601, -0.2242,  0.7402,  0.5145,  1.3520,  0.7402,  1.2245, -0.1221,\n",
       "         -0.6238, -0.5099, -0.0802, -0.7585, -0.5000,  0.5332,  0.7523, -0.5551,\n",
       "         -0.5922,  1.3873,  0.2179, -0.1776, -0.7997,  0.6600, -0.9766,  0.4366,\n",
       "         -0.0664,  0.5881,  0.3461, -0.8908, -0.5058, -0.6240, -0.2928, -1.1401,\n",
       "         -0.7249, -0.4957, -0.4429,  1.1751, -1.2238, -0.3252,  0.2941,  0.2219]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question_eng', 'question_urdu', 'context_eng', 'context_urdu', 'answer_eng', 'answer_urdu', 'context_index', '__index_level_0__'],\n",
      "        num_rows: 495\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question_eng', 'question_urdu', 'context_eng', 'context_urdu', 'answer_eng', 'answer_urdu', 'context_index', '__index_level_0__'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = load_from_disk(\"../../data/complete_dataset/\")\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_eng', 'question_urdu', 'context_eng', 'context_urdu', 'answer_eng', 'answer_urdu', 'context_index', '__index_level_0__'],\n",
       "    num_rows: 495\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[253,\n",
       " 271,\n",
       " 209,\n",
       " 266,\n",
       " 67,\n",
       " 167,\n",
       " 265,\n",
       " 13,\n",
       " 229,\n",
       " 255,\n",
       " 275,\n",
       " 50,\n",
       " 207,\n",
       " 218,\n",
       " 125,\n",
       " 79,\n",
       " 106,\n",
       " 257,\n",
       " 99,\n",
       " 163,\n",
       " 45,\n",
       " 284,\n",
       " 276,\n",
       " 242,\n",
       " 23,\n",
       " 171,\n",
       " 137,\n",
       " 30,\n",
       " 117,\n",
       " 187,\n",
       " 166,\n",
       " 305,\n",
       " 71,\n",
       " 22,\n",
       " 147,\n",
       " 62,\n",
       " 66,\n",
       " 193,\n",
       " 237,\n",
       " 152,\n",
       " 282,\n",
       " 101,\n",
       " 252,\n",
       " 172,\n",
       " 85,\n",
       " 285,\n",
       " 182,\n",
       " 232,\n",
       " 182,\n",
       " 83,\n",
       " 74,\n",
       " 42,\n",
       " 177,\n",
       " 190,\n",
       " 160,\n",
       " 215,\n",
       " 80,\n",
       " 254,\n",
       " 231,\n",
       " 219,\n",
       " 304,\n",
       " 227,\n",
       " 214,\n",
       " 246,\n",
       " 294,\n",
       " 98,\n",
       " 172,\n",
       " 161,\n",
       " 168,\n",
       " 306,\n",
       " 65,\n",
       " 226,\n",
       " 186,\n",
       " 291,\n",
       " 104,\n",
       " 19,\n",
       " 111,\n",
       " 226,\n",
       " 274,\n",
       " 115,\n",
       " 14,\n",
       " 0,\n",
       " 84,\n",
       " 99,\n",
       " 79,\n",
       " 179,\n",
       " 183,\n",
       " 284,\n",
       " 251,\n",
       " 148,\n",
       " 134,\n",
       " 92,\n",
       " 188,\n",
       " 259,\n",
       " 65,\n",
       " 157,\n",
       " 4,\n",
       " 208,\n",
       " 222,\n",
       " 220,\n",
       " 109,\n",
       " 121,\n",
       " 264,\n",
       " 302,\n",
       " 62,\n",
       " 71,\n",
       " 150,\n",
       " 250,\n",
       " 166,\n",
       " 234,\n",
       " 128,\n",
       " 6,\n",
       " 195,\n",
       " 201,\n",
       " 153,\n",
       " 257,\n",
       " 154,\n",
       " 59,\n",
       " 300,\n",
       " 159,\n",
       " 15,\n",
       " 64,\n",
       " 77,\n",
       " 189]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['validation']['context_urdu']\n",
    "dataset_dict['validation']['context_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contexts = {}\n",
    "\n",
    "for i in dataset_dict['train']:\n",
    "    all_contexts[i['context_index']] = i['context_urdu']\n",
    "\n",
    "\n",
    "for i in dataset_dict['validation']:\n",
    "    all_contexts[i['context_index']] = i['context_urdu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_contexts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urdu-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
